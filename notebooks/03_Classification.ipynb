{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Classification Models\n",
    "\n",
    "**DOST-ITDI AI Training Workshop**  \n",
    "**Day 1 - Session 3 (continued): Classification with Scikit-learn**\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "1. Understand classification problems in chemistry\n",
    "2. Implement classification algorithms\n",
    "3. Evaluate models using classification metrics\n",
    "4. Interpret confusion matrices and ROC curves\n",
    "5. Handle imbalanced datasets\n",
    "\n",
    "## What is Classification?\n",
    "\n",
    "Classification predicts **categorical labels** (classes).\n",
    "\n",
    "**Chemistry Applications**:\n",
    "- Drug activity (active/inactive)\n",
    "- Toxicity prediction (toxic/non-toxic)\n",
    "- Compound type classification\n",
    "- Quality control (pass/fail)\n",
    "- Reaction outcome (success/failure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries\n",
    "!pip install rdkit scikit-learn imbalanced-learn -q\n",
    "\n",
    "print(\"âœ“ Installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    ")\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotting\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BACE dataset (Blood-Brain Barrier Penetration)\n",
    "# Binary classification: active (1) vs inactive (0)\n",
    "url = \"https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/bace.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check target distribution - DETAILED ANALYSIS\nprint(\"=\"*60)\nprint(\"CLASS BALANCE ANALYSIS\")\nprint(\"=\"*60)\n\n# Absolute counts\nprint(\"\\n1. Class Counts:\")\nclass_counts = df['Class'].value_counts().sort_index()\nprint(class_counts)\n\n# Percentages\nprint(\"\\n2. Class Percentages:\")\nclass_percentages = df['Class'].value_counts(normalize=True).sort_index() * 100\nfor cls, pct in class_percentages.items():\n    print(f\"   Class {cls}: {pct:.2f}%\")\n\n# Ratio\nratio = class_counts[0] / class_counts[1]\nprint(f\"\\n3. Majority to Minority Ratio: {ratio:.2f}:1\")\n\n# Interpretation guidance\nprint(f\"\\n4. Interpretation:\")\nprint(f\"   This dataset has a ratio of {ratio:.2f}:1\")\nprint(f\"   Generally considered balanced (close to 1:1)\")\nprint(f\"   Imbalance becomes a concern when ratio > 3:1 or 5:1\")\nprint(f\"   Severe imbalance: 10:1, 100:1, or higher\")\nprint(f\"   (Thresholds depend on problem domain and cost of errors)\")\n\nprint(\"\\n\" + \"=\"*60)\n\n# Visualize\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Bar chart\nclass_counts.plot(kind='bar', ax=axes[0], color=['coral', 'steelblue'], \n                  edgecolor='black', alpha=0.7)\naxes[0].set_xlabel('Class (0=Inactive, 1=Active)', fontsize=12)\naxes[0].set_ylabel('Count', fontsize=12)\naxes[0].set_title('Class Distribution (Counts)', fontsize=14, fontweight='bold')\naxes[0].set_xticklabels(['Inactive (0)', 'Active (1)'], rotation=0)\naxes[0].grid(True, alpha=0.3, axis='y')\n\n# Add count labels on bars\nfor i, (idx, val) in enumerate(class_counts.items()):\n    axes[0].text(i, val + 20, str(val), ha='center', fontsize=12, fontweight='bold')\n\n# Pie chart\naxes[1].pie(class_counts.values, labels=['Inactive (0)', 'Active (1)'],\n            autopct='%1.1f%%', startangle=90, colors=['coral', 'steelblue'],\n            explode=[0.05, 0.05], shadow=True)\naxes[1].set_title('Class Distribution (Percentage)', fontsize=14, fontweight='bold')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute molecular descriptors\n",
    "df['mol'] = df['mol'].apply(Chem.MolFromSmiles)\n",
    "df = df[df['mol'].notna()].copy()\n",
    "\n",
    "# Calculate descriptors\n",
    "df['MolWeight'] = df['mol'].apply(Descriptors.MolWt)\n",
    "df['LogP'] = df['mol'].apply(Descriptors.MolLogP)\n",
    "df['NumHDonors'] = df['mol'].apply(Descriptors.NumHDonors)\n",
    "df['NumHAcceptors'] = df['mol'].apply(Descriptors.NumHAcceptors)\n",
    "df['NumRotatableBonds'] = df['mol'].apply(Descriptors.NumRotatableBonds)\n",
    "df['NumAromaticRings'] = df['mol'].apply(Descriptors.NumAromaticRings)\n",
    "df['TPSA'] = df['mol'].apply(Descriptors.TPSA)\n",
    "df['NumAtoms'] = df['mol'].apply(lambda x: x.GetNumAtoms())\n",
    "\n",
    "print(f\"Dataset after feature engineering: {df.shape[0]} molecules\")\n",
    "print(\"\\nNew features:\")\n",
    "print(df[['MolWeight', 'LogP', 'NumHDonors', 'NumHAcceptors']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature distribution by class\n",
    "features_to_plot = ['MolWeight', 'LogP', 'NumHDonors', 'TPSA']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(features_to_plot):\n",
    "    df.boxplot(column=feature, by='Class', ax=axes[idx], patch_artist=True)\n",
    "    axes[idx].set_xlabel('Class', fontsize=11)\n",
    "    axes[idx].set_ylabel(feature, fontsize=11)\n",
    "    axes[idx].set_title(f'{feature} by Class', fontsize=12, fontweight='bold')\n",
    "    axes[idx].get_figure().suptitle('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features and target\n",
    "feature_columns = ['MolWeight', 'LogP', 'NumHDonors', 'NumHAcceptors', \n",
    "                   'NumRotatableBonds', 'NumAromaticRings', 'TPSA', 'NumAtoms']\n",
    "\n",
    "X = df[feature_columns]\n",
    "y = df['Class']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y  # stratify maintains class balance\n",
    ")\n",
    "\n",
    "print(\"Dataset Split:\")\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"\\nTrain class distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\nTest class distribution:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"âœ“ Feature scaling complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Classification Models\n",
    "\n",
    "### 4.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression\n",
    "log_reg = LogisticRegression(random_state=42, max_iter=1000)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_lr = log_reg.predict(X_train_scaled)\n",
    "y_test_pred_lr = log_reg.predict(X_test_scaled)\n",
    "y_test_proba_lr = log_reg.predict_proba(X_test_scaled)[:, 1]  # Probability of class 1\n",
    "\n",
    "# Evaluate\n",
    "print(\"Logistic Regression Results:\")\n",
    "print(f\"Training Accuracy: {accuracy_score(y_train, y_train_pred_lr):.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_test_pred_lr):.4f}\")\n",
    "print(f\"Test Precision: {precision_score(y_test, y_test_pred_lr):.4f}\")\n",
    "print(f\"Test Recall: {recall_score(y_test, y_test_pred_lr):.4f}\")\n",
    "print(f\"Test F1-Score: {f1_score(y_test, y_test_pred_lr):.4f}\")\n",
    "print(f\"Test ROC-AUC: {roc_auc_score(y_test, y_test_proba_lr):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_rf = rf_clf.predict(X_train_scaled)\n",
    "y_test_pred_rf = rf_clf.predict(X_test_scaled)\n",
    "y_test_proba_rf = rf_clf.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "print(\"Random Forest Results:\")\n",
    "print(f\"Training Accuracy: {accuracy_score(y_train, y_train_pred_rf):.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_test_pred_rf):.4f}\")\n",
    "print(f\"Test Precision: {precision_score(y_test, y_test_pred_rf):.4f}\")\n",
    "print(f\"Test Recall: {recall_score(y_test, y_test_pred_rf):.4f}\")\n",
    "print(f\"Test F1-Score: {f1_score(y_test, y_test_pred_rf):.4f}\")\n",
    "print(f\"Test ROC-AUC: {roc_auc_score(y_test, y_test_proba_rf):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "feature_imp = pd.DataFrame({\n",
    "    'Feature': feature_columns,\n",
    "    'Importance': rf_clf.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_imp['Feature'], feature_imp['Importance'], \n",
    "         color='steelblue', alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Importance Score', fontsize=12)\n",
    "plt.title('Random Forest: Feature Importance', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Gradient Boosting\n",
    "gb_clf = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "gb_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_gb = gb_clf.predict(X_train_scaled)\n",
    "y_test_pred_gb = gb_clf.predict(X_test_scaled)\n",
    "y_test_proba_gb = gb_clf.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "print(\"Gradient Boosting Results:\")\n",
    "print(f\"Training Accuracy: {accuracy_score(y_train, y_train_pred_gb):.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_test_pred_gb):.4f}\")\n",
    "print(f\"Test Precision: {precision_score(y_test, y_test_pred_gb):.4f}\")\n",
    "print(f\"Test Recall: {recall_score(y_test, y_test_pred_gb):.4f}\")\n",
    "print(f\"Test F1-Score: {f1_score(y_test, y_test_pred_gb):.4f}\")\n",
    "print(f\"Test ROC-AUC: {roc_auc_score(y_test, y_test_proba_gb):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile results\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Random Forest', 'Gradient Boosting'],\n",
    "    'Accuracy': [\n",
    "        accuracy_score(y_test, y_test_pred_lr),\n",
    "        accuracy_score(y_test, y_test_pred_rf),\n",
    "        accuracy_score(y_test, y_test_pred_gb)\n",
    "    ],\n",
    "    'Precision': [\n",
    "        precision_score(y_test, y_test_pred_lr),\n",
    "        precision_score(y_test, y_test_pred_rf),\n",
    "        precision_score(y_test, y_test_pred_gb)\n",
    "    ],\n",
    "    'Recall': [\n",
    "        recall_score(y_test, y_test_pred_lr),\n",
    "        recall_score(y_test, y_test_pred_rf),\n",
    "        recall_score(y_test, y_test_pred_gb)\n",
    "    ],\n",
    "    'F1-Score': [\n",
    "        f1_score(y_test, y_test_pred_lr),\n",
    "        f1_score(y_test, y_test_pred_rf),\n",
    "        f1_score(y_test, y_test_pred_gb)\n",
    "    ],\n",
    "    'ROC-AUC': [\n",
    "        roc_auc_score(y_test, y_test_proba_lr),\n",
    "        roc_auc_score(y_test, y_test_proba_rf),\n",
    "        roc_auc_score(y_test, y_test_proba_gb)\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "print(results.to_string(index=False))\n",
    "\n",
    "# Best model\n",
    "best_model_idx = results['ROC-AUC'].idxmax()\n",
    "print(f\"\\nðŸ† Best Model: {results.loc[best_model_idx, 'Model']}\")\n",
    "print(f\"   ROC-AUC: {results.loc[best_model_idx, 'ROC-AUC']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Metrics comparison\n",
    "metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "x = np.arange(len(results))\n",
    "width = 0.2\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    axes[0].bar(x + i*width, results[metric], width, label=metric, alpha=0.8)\n",
    "\n",
    "axes[0].set_xlabel('Model', fontsize=12)\n",
    "axes[0].set_ylabel('Score', fontsize=12)\n",
    "axes[0].set_title('Classification Metrics Comparison', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xticks(x + width * 1.5)\n",
    "axes[0].set_xticklabels(results['Model'], rotation=45, ha='right')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# ROC-AUC comparison\n",
    "axes[1].bar(results['Model'], results['ROC-AUC'], \n",
    "            color='coral', alpha=0.8, edgecolor='black')\n",
    "axes[1].set_xlabel('Model', fontsize=12)\n",
    "axes[1].set_ylabel('ROC-AUC Score', fontsize=12)\n",
    "axes[1].set_title('ROC-AUC Comparison', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xticklabels(results['Model'], rotation=45, ha='right')\n",
    "axes[1].set_ylim([0.5, 1.0])\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices for all models\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "predictions = [\n",
    "    ('Logistic Regression', y_test_pred_lr),\n",
    "    ('Random Forest', y_test_pred_rf),\n",
    "    ('Gradient Boosting', y_test_pred_gb)\n",
    "]\n",
    "\n",
    "for idx, (name, y_pred) in enumerate(predictions):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=axes[idx],\n",
    "                xticklabels=['Inactive', 'Active'],\n",
    "                yticklabels=['Inactive', 'Active'])\n",
    "    axes[idx].set_xlabel('Predicted', fontsize=11)\n",
    "    axes[idx].set_ylabel('Actual', fontsize=11)\n",
    "    axes[idx].set_title(f'{name}', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed confusion matrix explanation for Random Forest\n",
    "cm_rf = confusion_matrix(y_test, y_test_pred_rf)\n",
    "\n",
    "tn, fp, fn, tp = cm_rf.ravel()\n",
    "\n",
    "print(\"Confusion Matrix Breakdown (Random Forest):\")\n",
    "print(f\"True Negatives (TN): {tn} - Correctly predicted as Inactive\")\n",
    "print(f\"False Positives (FP): {fp} - Incorrectly predicted as Active\")\n",
    "print(f\"False Negatives (FN): {fn} - Incorrectly predicted as Inactive\")\n",
    "print(f\"True Positives (TP): {tp} - Correctly predicted as Active\")\n",
    "print(f\"\\nTotal predictions: {tn + fp + fn + tp}\")\n",
    "print(f\"Correct predictions: {tn + tp} ({(tn + tp)/(tn + fp + fn + tp)*100:.2f}%)\")\n",
    "print(f\"Incorrect predictions: {fp + fn} ({(fp + fn)/(tn + fp + fn + tp)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ROC Curve Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves for all models\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "models_proba = [\n",
    "    ('Logistic Regression', y_test_proba_lr),\n",
    "    ('Random Forest', y_test_proba_rf),\n",
    "    ('Gradient Boosting', y_test_proba_gb)\n",
    "]\n",
    "\n",
    "for name, y_proba in models_proba:\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=f'{name} (AUC = {auc:.3f})')\n",
    "\n",
    "# Random classifier line\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier (AUC = 0.5)')\n",
    "\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curves - Model Comparison', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report for Random Forest\n",
    "print(\"Classification Report (Random Forest):\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_test, y_test_pred_rf, \n",
    "                           target_names=['Inactive (0)', 'Active (1)']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Understanding Classification Metrics\n",
    "\n",
    "### Key Metrics Explained:\n",
    "\n",
    "1. **Accuracy**: (TP + TN) / Total\n",
    "   - Overall correctness\n",
    "   - Can be misleading with imbalanced data\n",
    "\n",
    "2. **Precision**: TP / (TP + FP)\n",
    "   - Of all positive predictions, how many are correct?\n",
    "   - Important when false positives are costly\n",
    "\n",
    "3. **Recall (Sensitivity)**: TP / (TP + FN)\n",
    "   - Of all actual positives, how many did we find?\n",
    "   - Important when false negatives are costly\n",
    "\n",
    "4. **F1-Score**: 2 Ã— (Precision Ã— Recall) / (Precision + Recall)\n",
    "   - Harmonic mean of precision and recall\n",
    "   - Balanced measure\n",
    "\n",
    "5. **ROC-AUC**: Area Under the ROC Curve\n",
    "   - Overall model discrimination ability\n",
    "   - Threshold-independent\n",
    "\n",
    "### When to use which metric?\n",
    "\n",
    "- **Drug Discovery**: High recall (don't miss active compounds)\n",
    "- **Toxicity Screening**: High precision (avoid false alarms)\n",
    "- **Quality Control**: Balance with F1-score\n",
    "- **Model Comparison**: ROC-AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Prediction Threshold Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different thresholds\n",
    "thresholds = np.arange(0.1, 1.0, 0.05)\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_threshold = (y_test_proba_rf >= threshold).astype(int)\n",
    "    precision_scores.append(precision_score(y_test, y_pred_threshold))\n",
    "    recall_scores.append(recall_score(y_test, y_pred_threshold))\n",
    "    f1_scores.append(f1_score(y_test, y_pred_threshold))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, precision_scores, 'o-', label='Precision', linewidth=2)\n",
    "plt.plot(thresholds, recall_scores, 's-', label='Recall', linewidth=2)\n",
    "plt.plot(thresholds, f1_scores, '^-', label='F1-Score', linewidth=2)\n",
    "plt.axvline(x=0.5, color='gray', linestyle='--', linewidth=1, label='Default (0.5)')\n",
    "plt.xlabel('Classification Threshold', fontsize=12)\n",
    "plt.ylabel('Score', fontsize=12)\n",
    "plt.title('Metrics vs Classification Threshold', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Find optimal threshold for F1\n",
    "optimal_idx = np.argmax(f1_scores)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "print(f\"\\nOptimal threshold for F1-Score: {optimal_threshold:.2f}\")\n",
    "print(f\"F1-Score at optimal threshold: {f1_scores[optimal_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample predictions\n",
    "sample_indices = [0, 10, 20, 30, 40]\n",
    "samples = X_test.iloc[sample_indices]\n",
    "samples_scaled = scaler.transform(samples)\n",
    "\n",
    "# Predict with probabilities\n",
    "predictions = rf_clf.predict(samples_scaled)\n",
    "probabilities = rf_clf.predict_proba(samples_scaled)\n",
    "actual = y_test.iloc[sample_indices].values\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame({\n",
    "    'Actual': ['Active' if x == 1 else 'Inactive' for x in actual],\n",
    "    'Predicted': ['Active' if x == 1 else 'Inactive' for x in predictions],\n",
    "    'Prob_Inactive': probabilities[:, 0],\n",
    "    'Prob_Active': probabilities[:, 1],\n",
    "    'Correct': actual == predictions\n",
    "})\n",
    "\n",
    "print(\"\\nSample Predictions:\")\n",
    "print(results_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Create Imbalanced Baseline Dataset\nfrom imblearn.under_sampling import RandomUnderSampler\n\nprint(\"=\"*60)\nprint(\"CREATING IMBALANCED BASELINE DATASET\")\nprint(\"=\"*60)\n\nprint(\"\\nOriginal Training Data:\")\nprint(f\"Training set size: {X_train_scaled.shape[0]}\")\nclass_counts_before = y_train.value_counts().sort_index()\nprint(f\"Class distribution:\\n{class_counts_before}\")\nratio_before = class_counts_before[0] / class_counts_before[1]\nprint(f\"Ratio: {ratio_before:.2f}:1 (relatively balanced)\")\n\n# Apply under-sampling - Remove 50% of majority class (Class 0)\n# This creates an imbalanced dataset for demonstrating handling techniques\nmajority_class = class_counts_before.idxmax()\nmajority_count = class_counts_before[majority_class]\ntarget_majority_count = int(majority_count * 0.5)\n\nprint(f\"\\nCreating Imbalance: Remove 50% of Class {majority_class}\")\nprint(f\"  Original: {majority_count} samples\")\nprint(f\"  Target: {target_majority_count} samples\")\nprint(f\"  Removing: {majority_count - target_majority_count} samples\")\n\nrus = RandomUnderSampler(sampling_strategy={majority_class: target_majority_count}, random_state=42)\nX_train_rus, y_train_rus = rus.fit_resample(X_train_scaled, y_train)\n\nprint(f\"\\nImbalanced Baseline Dataset Created:\")\nprint(f\"Training set size: {X_train_rus.shape[0]}\")\nclass_counts_after = pd.Series(y_train_rus).value_counts().sort_index()\nprint(f\"Class distribution:\\n{class_counts_after}\")\nratio_after = class_counts_after[1] / class_counts_after[0]\nprint(f\"Ratio: {ratio_after:.2f}:1 (Class 1 is now majority)\")\nprint(f\"\\nThis imbalanced dataset will be used to demonstrate\")\nprint(f\"different techniques for handling class imbalance.\")\nprint(\"=\"*60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Compare all techniques\nprint(\"\\n\" + \"=\"*60)\nprint(\"COMPARISON OF ALL TECHNIQUES\")\nprint(\"=\"*60)\n\nprint(\"\\nAll models trained on same imbalanced baseline (328:553)\")\nprint(\"Evaluated on same test set\\n\")\n\ntechniques = {\n    'No Handling': y_pred_default,\n    'Class Weights': y_pred_balanced,\n    'SMOTE': y_pred_smote\n}\n\ncomparison_results = []\n\nfor name, y_pred in techniques.items():\n    comparison_results.append({\n        'Technique': name,\n        'Accuracy': accuracy_score(y_test, y_pred),\n        'Precision': precision_score(y_test, y_pred),\n        'Recall': recall_score(y_test, y_pred),\n        'F1-Score': f1_score(y_test, y_pred)\n    })\n\ncomparison_df = pd.DataFrame(comparison_results)\nprint(\"\\nComparison of Imbalance Handling Techniques:\")\nprint(\"=\"*80)\nprint(comparison_df.to_string(index=False))\n\n# Visualize\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\n# Precision vs Recall scatter\naxes[0].scatter(comparison_df['Recall'], comparison_df['Precision'],\n               s=200, alpha=0.6, c=range(len(comparison_df)),\n               cmap='viridis', edgecolors='black', linewidth=2)\nfor idx, row in comparison_df.iterrows():\n    axes[0].annotate(row['Technique'],\n                    (row['Recall'], row['Precision']),\n                    xytext=(5, 5), textcoords='offset points', fontsize=9)\naxes[0].set_xlabel('Recall', fontsize=12)\naxes[0].set_ylabel('Precision', fontsize=12)\naxes[0].set_title('Precision vs Recall Trade-off', fontsize=13, fontweight='bold')\naxes[0].grid(True, alpha=0.3)\n\n# F1-Score comparison\naxes[1].barh(comparison_df['Technique'], comparison_df['F1-Score'],\n            color='steelblue', alpha=0.7, edgecolor='black')\naxes[1].set_xlabel('F1-Score', fontsize=12)\naxes[1].set_title('F1-Score by Technique', fontsize=13, fontweight='bold')\naxes[1].grid(True, alpha=0.3, axis='x')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nKey Insights:\")\nprint(\"- All techniques trained on same imbalanced baseline (328:553 = 1.69:1 ratio)\")\nprint(\"- SMOTE and Class Weights help handle the imbalance\")\nprint(\"- No Handling shows baseline performance without any technique\")\nprint(\"- Compare Recall for minority class (Class 0) - higher is better\")\nprint(\"- For more severe imbalance (10:1, 100:1), differences would be larger\")\nprint(\"=\"*60)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### 12.5 Summary: Which Technique to Use?\n\n| Technique | Pros | Cons | Best For |\n|-----------|------|------|----------|\n| **Class Weights** | Simple, no data change, fast | Limited for severe imbalance | First approach, moderate imbalance |\n| **SMOTE** | Generates new data, maintains info | Can create noise | Good minority samples, SVM/NN |\n| **Under-sampling** | Fast, reduces size | Loses information | Large datasets, time constraints |\n\n**General Recommendations:**\n\n1. **Start Simple:** Try class weights first\n2. **Moderate Imbalance (3:1 to 10:1):** Class weights or SMOTE\n3. **Severe Imbalance (>10:1):** SMOTE or combination methods\n4. **Huge Dataset:** Under-sampling acceptable\n5. **Always:** Use stratified cross-validation\n6. **Metrics:** Focus on F1, precision, recall - not accuracy!\n\n**For Chemistry/Drug Discovery:**\n- **Toxicity screening:** Prioritize recall (don't miss toxic compounds)\n- **Hit identification:** Balance precision/recall with F1-score\n- **Rare effects:** Definitely need resampling techniques",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### 12.4 Comparing All Techniques",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Technique 1: SMOTE (Synthetic Minority Over-sampling Technique)\nfrom imblearn.over_sampling import SMOTE\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"TECHNIQUE 1: SMOTE\")\nprint(\"=\"*60)\n\nprint(\"\\nStarting from Imbalanced Baseline:\")\nprint(f\"Training set size: {X_train_rus.shape[0]}\")\nbaseline_counts = pd.Series(y_train_rus).value_counts().sort_index()\nprint(f\"Class distribution:\\n{baseline_counts}\")\nprint(f\"Ratio: {baseline_counts[1]/baseline_counts[0]:.2f}:1 (imbalanced)\")\n\n# Apply SMOTE to imbalanced baseline\nprint(\"\\nApplying SMOTE...\")\nsmote = SMOTE(random_state=42)\nX_train_smote, y_train_smote = smote.fit_resample(X_train_rus, y_train_rus)\n\nprint(f\"\\nAfter SMOTE:\")\nprint(f\"Training set size: {X_train_smote.shape[0]}\")\nprint(f\"Class distribution:\\n{pd.Series(y_train_smote).value_counts()}\")\nprint(f\"Ratio: 1:1 (perfectly balanced)\")\n\n# Train model on SMOTE data\nrf_smote = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_smote.fit(X_train_smote, y_train_smote)\ny_pred_smote = rf_smote.predict(X_test_scaled)\n\nprint(\"\\nModel Performance with SMOTE:\")\nprint(classification_report(y_test, y_pred_smote,\n                           target_names=['Inactive', 'Active']))\n\n# Visualize SMOTE effect (2D projection)\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\nX_baseline_pca = pca.fit_transform(X_train_rus)\nX_smote_pca = pca.transform(X_train_smote)\n\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\n# Imbalanced Baseline\nfor class_val in [0, 1]:\n    mask = y_train_rus == class_val\n    axes[0].scatter(X_baseline_pca[mask, 0], X_baseline_pca[mask, 1],\n                   label=f'Class {class_val}', alpha=0.6, s=30)\naxes[0].set_title('Imbalanced Baseline (328:553)', fontsize=13, fontweight='bold')\naxes[0].set_xlabel('First Principal Component')\naxes[0].set_ylabel('Second Principal Component')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# After SMOTE\nfor class_val in [0, 1]:\n    mask = y_train_smote == class_val\n    axes[1].scatter(X_smote_pca[mask, 0], X_smote_pca[mask, 1],\n                   label=f'Class {class_val}', alpha=0.6, s=30)\naxes[1].set_title('After SMOTE (Balanced)', fontsize=13, fontweight='bold')\naxes[1].set_xlabel('First Principal Component')\naxes[1].set_ylabel('Second Principal Component')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\nprint(\"=\"*60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 12.3 Technique 3: Random Under-sampling\n\nRemove samples from majority class to balance with minority class.\n\n**How it works:**\n- Randomly remove majority class samples\n- Until both classes have similar size\n\n**When to use:**\n- Very large datasets where losing data is acceptable\n- When training time is critical\n- When majority class has redundant information\n\n**Caution:**\n- Loses potentially useful information\n- May hurt model performance\n- Only use if you have lots of majority samples",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Technique 1: SMOTE (Synthetic Minority Over-sampling Technique)\n\nGenerate synthetic samples of minority class by interpolating between existing samples.\n\n**How it works:**\n1. Select a minority sample\n2. Find its k nearest neighbors\n3. Create new sample between original and neighbor\n4. Repeat until classes balanced\n\n**When to use:**\n- Moderate to severe imbalance\n- When you need more training data\n- Works well with SVM, Neural Networks\n\n**In our demo:**\n- Starts with imbalanced baseline (328:553)\n- SMOTE creates synthetic samples for Class 0 (minority)\n- Results in perfectly balanced dataset (553:553)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 12.2 Technique 2: SMOTE (Synthetic Minority Over-sampling Technique)\n\nGenerate synthetic samples of minority class by interpolating between existing samples.\n\n**How it works:**\n1. Select a minority sample\n2. Find its k nearest neighbors\n3. Create new sample between original and neighbor\n4. Repeat until classes balanced\n\n**When to use:**\n- Moderate to severe imbalance\n- When you need more training data\n- Works well with SVM, Neural Networks\n\n**Caution:**\n- May create synthetic samples in noisy regions\n- Can lead to overfitting if overused",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Technique 2 & 3: No Handling (Baseline) and Class Weights\n\n**Technique 2: No Handling (Baseline)**\n- Train model directly on imbalanced data\n- No special techniques applied\n- Shows baseline performance for comparison\n\n**Technique 3: Class Weights**\nAutomatically adjust model to penalize misclassification of minority class more heavily.\n\n**How it works:**\n- Assigns higher weight to minority class\n- Model pays more attention to minority samples during training\n- No data modification needed\n- Formula: weight = n_samples / (n_classes * class_count)\n\n**When to use:**\n- First approach to try\n- Moderate imbalance (up to 5:1)\n- When you want to keep original data\n\n**In our demo:**\n- Both techniques use imbalanced baseline (328:553)\n- Class weights: Class 0 gets ~1.34x weight, Class 1 gets ~0.80x weight\n- Minority class errors penalized more during training",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 12.1 Technique 1: Class Weights\n\nAutomatically adjust model to penalize misclassification of minority class more heavily.\n\n**How it works:**\n- Assigns higher weight to minority class\n- Model pays more attention to minority samples during training\n- No data modification needed\n\n**When to use:**\n- First approach to try\n- Moderate imbalance (up to 5:1)\n- When you want to keep original data",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 12. Handling Imbalanced Datasets (Reference Guide)\n\n### Note on This Dataset\n\nOur BACE dataset is relatively balanced (54% vs 46%). However, in real-world chemistry applications, you'll often encounter imbalanced datasets:\n\n**Common Imbalanced Scenarios:**\n- Toxicity screening: 95% non-toxic, 5% toxic\n- Rare adverse effects: 99% safe, 1% adverse\n- Hit identification: 98% inactive, 2% active\n- Quality control: 97% pass, 3% fail\n\nThis section shows techniques for handling such cases.\n\n### What is Class Imbalance?\n\nClass imbalance occurs when one class has significantly more samples than another.\n\n**Problems it causes:**\n- Model biased toward majority class\n- Can achieve high accuracy by always predicting majority\n- Poor detection of minority class (often the important one!)\n- Misleading accuracy metric",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 12. Handling Imbalanced Datasets\n\n### What is Class Imbalance?\n\nClass imbalance occurs when one class has significantly more samples than another.\n\n**Example:**\n- Toxicity: 95% non-toxic, 5% toxic\n- Rare diseases: 99% negative, 1% positive\n- Quality control: 98% pass, 2% fail\n\n**Problems:**\n- Model biased toward majority class\n- High accuracy but poor minority class detection\n- Misleading performance metrics",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary and Best Practices\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Data Preparation**\n",
    "   - Check class balance\n",
    "   - Use stratified split\n",
    "   - Scale features appropriately\n",
    "\n",
    "2. **Model Selection**\n",
    "   - Start simple (Logistic Regression)\n",
    "   - Try ensemble methods (Random Forest, Gradient Boosting)\n",
    "   - Compare multiple models\n",
    "\n",
    "3. **Evaluation**\n",
    "   - Don't rely on accuracy alone\n",
    "   - Use confusion matrix\n",
    "   - Consider precision/recall trade-off\n",
    "   - Use ROC-AUC for overall performance\n",
    "\n",
    "4. **Chemistry-Specific Considerations**\n",
    "   - Feature importance reveals key molecular properties\n",
    "   - Threshold optimization depends on use case\n",
    "   - Consider cost of false positives vs false negatives\n",
    "\n",
    "### Next Steps:\n",
    "- Deep Learning with PyTorch\n",
    "- Neural networks for molecular data\n",
    "- Transfer learning with transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try the following:\n",
    "# 1. Train a Support Vector Machine (SVC) classifier\n",
    "# 2. Compare its performance with the models above\n",
    "# 3. Try adjusting the classification threshold\n",
    "# 4. Create a confusion matrix for your model\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Scikit-learn Classification Guide](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning)\n",
    "- [Understanding ROC Curves](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)\n",
    "- [Precision vs Recall](https://en.wikipedia.org/wiki/Precision_and_recall)\n",
    "\n",
    "**Next Notebook: Deep Learning with PyTorch**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}